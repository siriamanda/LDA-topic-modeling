{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling and Latent Dirichlet Allocation (LDA) in Python\n",
    "\n",
    "Source: Susan Li https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "\n",
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines = False)    # Bad lines will be dropped from the dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe 'documents'\n",
    "\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  index\n",
       "0  aba decides against community broadcasting lic...      0\n",
       "1     act fire witnesses must be aware of defamation      1\n",
       "2     a g calls for infrastructure protection summit      2\n",
       "3           air nz staff in aust strike for pay rise      3\n",
       "4      air nz strike to affect australian travellers      4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1226258, 2)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform preprocessing steps: \n",
    "# Removing tokens less than 3 characters, remove stopwords, lemmatization and stemming\n",
    "\n",
    "def lemmatize(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos = 'v')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample and preview after preprocessing \n",
    "\n",
    "doc_sample = documents[documents['index'] == 4310].values[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ratepayers group wants compulsory local govt voting'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document:  ['ratepayers', 'group', 'wants', 'compulsory', 'local', 'govt', 'voting']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for word in doc_sample.split(' '):      # Tokenize sentence\n",
    "    words.append(word)\n",
    "print('original document: ', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['ratepayers', 'group', 'want', 'compulsory', 'local', 'govt', 'vote']\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              [decide, community, broadcast, licence]\n",
       "1                         [witness, aware, defamation]\n",
       "2           [call, infrastructure, protection, summit]\n",
       "3                          [staff, aust, strike, rise]\n",
       "4             [strike, affect, australian, travellers]\n",
       "5               [ambitious, olsson, win, triple, jump]\n",
       "6               [antic, delight, record, break, barca]\n",
       "7    [aussie, qualifier, stosur, waste, memphis, ma...\n",
       "8             [aust, address, security, council, iraq]\n",
       "9                         [australia, lock, timetable]\n",
       "Name: headline_text, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess all headline texts\n",
    "\n",
    "processed_docs = documents['headline_text'].map(preprocess)\n",
    "processed_docs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 broadcast\n",
      "1 community\n",
      "2 decide\n",
      "3 licence\n",
      "4 aware\n",
      "5 defamation\n",
      "6 witness\n",
      "7 call\n",
      "8 infrastructure\n",
      "9 protection\n",
      "10 summit\n"
     ]
    }
   ],
   "source": [
    "# Bag of words on the Data set\n",
    "# Create a dictionary from 'processed_docs' containing the number of times a word appears in the training set\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim filter extremes\n",
    "\n",
    "# Filter out tokens that appear in:\n",
    "# Less than 15 documents, more than 0.5 documents, and keep only 100000 most frquent tokens\n",
    "\n",
    "dictionary.filter_extremes(no_below = 15, no_above = 0.5, keep_n = 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW\n",
    "\n",
    "A bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n",
    "\n",
    "A vocabulary of known words.\n",
    "A measure of the presence of known words.\n",
    "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document.\n",
    "\n",
    "https://machinelearningmastery.com/gentle-introduction-bag-words-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(164, 1), (241, 1), (295, 1), (600, 1), (864, 1), (3915, 1), (3916, 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gensim doc2bow\n",
    "# Create a dictionary reporting how many words and how many times those words appear\n",
    "\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 164 govt') appears 1 time.\n",
      "Word 241 group') appears 1 time.\n",
      "Word 295 vote') appears 1 time.\n",
      "Word 600 local') appears 1 time.\n",
      "Word 864 want') appears 1 time.\n",
      "Word 3915 compulsory') appears 1 time.\n",
      "Word 3916 ratepayers') appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# Preview Bag Of Words for our sample preprocessed document\n",
    "\n",
    "bow_doc_4310 = bow_corpus[4310]\n",
    "\n",
    "for i in range (len(bow_doc_4310)):\n",
    "    print('Word {} {}\\') appears {} time.'.format(bow_doc_4310[i][0],\n",
    "                        dictionary[bow_doc_4310[i][0]], \n",
    "                                bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5918674193999763),\n",
      " (1, 0.3937180767686992),\n",
      " (2, 0.5009876624450964),\n",
      " (3, 0.49365007440105513)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)      # Create a TF-IDF model\n",
    "corpus_tfidf = tfidf[bow_corpus]           # Apply transformation to the entire corpus and call it ‘corpus_tfidf’\n",
    "\n",
    "for doc in corpus_tfidf:       # Preview for the first document\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using bag of words\n",
    "\n",
    "# Define the model\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = 10, id2word = dictionary, \n",
    "                                      passes = 2, workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.059*\"australia\" + 0.034*\"coronavirus\" + 0.024*\"australian\" + 0.023*\"china\" + 0.020*\"sydney\" + 0.019*\"world\" + 0.017*\"open\" + 0.016*\"border\" + 0.012*\"win\" + 0.011*\"take\"\n",
      "Topic: 1 \n",
      "Words: 0.022*\"market\" + 0.021*\"record\" + 0.019*\"year\" + 0.013*\"care\" + 0.012*\"years\" + 0.012*\"price\" + 0.011*\"australian\" + 0.011*\"business\" + 0.011*\"country\" + 0.010*\"age\"\n",
      "Topic: 2 \n",
      "Words: 0.043*\"coronavirus\" + 0.030*\"government\" + 0.025*\"covid\" + 0.015*\"restrictions\" + 0.015*\"rise\" + 0.014*\"water\" + 0.012*\"royal\" + 0.012*\"scott\" + 0.012*\"tasmanian\" + 0.011*\"say\"\n",
      "Topic: 3 \n",
      "Words: 0.026*\"kill\" + 0.022*\"die\" + 0.018*\"coast\" + 0.018*\"shoot\" + 0.017*\"miss\" + 0.016*\"crash\" + 0.015*\"attack\" + 0.014*\"gold\" + 0.014*\"dead\" + 0.013*\"island\"\n",
      "Topic: 4 \n",
      "Words: 0.040*\"police\" + 0.027*\"case\" + 0.026*\"charge\" + 0.026*\"court\" + 0.020*\"death\" + 0.020*\"murder\" + 0.017*\"face\" + 0.013*\"jail\" + 0.013*\"people\" + 0.012*\"arrest\"\n",
      "Topic: 5 \n",
      "Words: 0.056*\"trump\" + 0.028*\"test\" + 0.020*\"tasmania\" + 0.015*\"morrison\" + 0.014*\"drum\" + 0.012*\"premier\" + 0.011*\"save\" + 0.010*\"race\" + 0.009*\"show\" + 0.009*\"park\"\n",
      "Topic: 6 \n",
      "Words: 0.037*\"say\" + 0.025*\"victoria\" + 0.022*\"health\" + 0.019*\"change\" + 0.018*\"adelaide\" + 0.014*\"indigenous\" + 0.010*\"rural\" + 0.010*\"service\" + 0.010*\"covid\" + 0.010*\"national\"\n",
      "Topic: 7 \n",
      "Words: 0.036*\"queensland\" + 0.027*\"election\" + 0.022*\"live\" + 0.013*\"federal\" + 0.012*\"work\" + 0.012*\"school\" + 0.010*\"council\" + 0.010*\"farm\" + 0.009*\"fund\" + 0.009*\"vote\"\n",
      "Topic: 8 \n",
      "Words: 0.019*\"bushfire\" + 0.016*\"speak\" + 0.015*\"final\" + 0.013*\"beat\" + 0.013*\"street\" + 0.012*\"season\" + 0.012*\"wall\" + 0.011*\"john\" + 0.010*\"young\" + 0.010*\"continue\"\n",
      "Topic: 9 \n",
      "Words: 0.026*\"donald\" + 0.025*\"south\" + 0.024*\"news\" + 0.017*\"north\" + 0.016*\"perth\" + 0.015*\"time\" + 0.013*\"victorian\" + 0.013*\"return\" + 0.012*\"interview\" + 0.012*\"west\"\n"
     ]
    }
   ],
   "source": [
    "# Explore the words occurring in that topic and its relative weight\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the LDA using TD-IDF\n",
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics = 10, id2word = dictionary, \n",
    "                                             passes = 2, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.026*\"trump\" + 0.012*\"drum\" + 0.010*\"australia\" + 0.009*\"world\" + 0.008*\"scott\" + 0.008*\"final\" + 0.008*\"tuesday\" + 0.007*\"league\" + 0.006*\"cricket\" + 0.006*\"test\"\n",
      "Topic: 1 Word: 0.013*\"hour\" + 0.009*\"david\" + 0.008*\"pandemic\" + 0.008*\"march\" + 0.008*\"plead\" + 0.007*\"june\" + 0.006*\"dollar\" + 0.006*\"reopen\" + 0.005*\"sunday\" + 0.005*\"suicide\"\n",
      "Topic: 2 Word: 0.023*\"coronavirus\" + 0.015*\"covid\" + 0.012*\"donald\" + 0.009*\"queensland\" + 0.009*\"restrictions\" + 0.008*\"victoria\" + 0.006*\"rain\" + 0.006*\"australia\" + 0.006*\"farmers\" + 0.006*\"drought\"\n",
      "Topic: 3 Word: 0.016*\"police\" + 0.015*\"charge\" + 0.013*\"crash\" + 0.012*\"murder\" + 0.012*\"kill\" + 0.011*\"woman\" + 0.010*\"court\" + 0.009*\"shoot\" + 0.008*\"arrest\" + 0.008*\"die\"\n",
      "Topic: 4 Word: 0.009*\"morrison\" + 0.009*\"health\" + 0.009*\"care\" + 0.007*\"age\" + 0.007*\"coronavirus\" + 0.006*\"mental\" + 0.005*\"government\" + 0.005*\"legal\" + 0.004*\"say\" + 0.004*\"remote\"\n",
      "Topic: 5 Word: 0.010*\"coronavirus\" + 0.009*\"monday\" + 0.008*\"michael\" + 0.008*\"miss\" + 0.007*\"search\" + 0.007*\"lockdown\" + 0.006*\"covid\" + 0.006*\"vaccine\" + 0.006*\"stories\" + 0.005*\"september\"\n",
      "Topic: 6 Word: 0.015*\"interview\" + 0.011*\"friday\" + 0.010*\"finance\" + 0.010*\"update\" + 0.008*\"violence\" + 0.007*\"history\" + 0.007*\"grandstand\" + 0.007*\"domestic\" + 0.007*\"extend\" + 0.006*\"october\"\n",
      "Topic: 7 Word: 0.015*\"country\" + 0.015*\"coast\" + 0.013*\"live\" + 0.010*\"gold\" + 0.010*\"north\" + 0.008*\"christmas\" + 0.007*\"turnbull\" + 0.007*\"peter\" + 0.007*\"alan\" + 0.007*\"korea\"\n",
      "Topic: 8 Word: 0.017*\"news\" + 0.010*\"rural\" + 0.010*\"election\" + 0.008*\"government\" + 0.007*\"federal\" + 0.007*\"wednesday\" + 0.007*\"thursday\" + 0.007*\"business\" + 0.007*\"national\" + 0.007*\"john\"\n",
      "Topic: 9 Word: 0.006*\"kohler\" + 0.006*\"government\" + 0.006*\"jam\" + 0.005*\"people\" + 0.005*\"school\" + 0.005*\"coronavirus\" + 0.005*\"online\" + 0.005*\"territory\" + 0.005*\"sell\" + 0.005*\"smith\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ratepayers', 'group', 'want', 'compulsory', 'local', 'govt', 'vote']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "\n",
    "# Checking where our test document would be classified\n",
    "\n",
    "processed_docs[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5026649236679077\t \n",
      "Topic: 0.043*\"coronavirus\" + 0.030*\"government\" + 0.025*\"covid\" + 0.015*\"restrictions\" + 0.015*\"rise\" + 0.014*\"water\" + 0.012*\"royal\" + 0.012*\"scott\" + 0.012*\"tasmanian\" + 0.011*\"say\"\n",
      "\n",
      "Score: 0.3973022997379303\t \n",
      "Topic: 0.036*\"queensland\" + 0.027*\"election\" + 0.022*\"live\" + 0.013*\"federal\" + 0.012*\"work\" + 0.012*\"school\" + 0.010*\"council\" + 0.010*\"farm\" + 0.009*\"fund\" + 0.009*\"vote\"\n",
      "\n",
      "Score: 0.012505652382969856\t \n",
      "Topic: 0.037*\"say\" + 0.025*\"victoria\" + 0.022*\"health\" + 0.019*\"change\" + 0.018*\"adelaide\" + 0.014*\"indigenous\" + 0.010*\"rural\" + 0.010*\"service\" + 0.010*\"covid\" + 0.010*\"national\"\n",
      "\n",
      "Score: 0.01250387355685234\t \n",
      "Topic: 0.059*\"australia\" + 0.034*\"coronavirus\" + 0.024*\"australian\" + 0.023*\"china\" + 0.020*\"sydney\" + 0.019*\"world\" + 0.017*\"open\" + 0.016*\"border\" + 0.012*\"win\" + 0.011*\"take\"\n",
      "\n",
      "Score: 0.01250387355685234\t \n",
      "Topic: 0.022*\"market\" + 0.021*\"record\" + 0.019*\"year\" + 0.013*\"care\" + 0.012*\"years\" + 0.012*\"price\" + 0.011*\"australian\" + 0.011*\"business\" + 0.011*\"country\" + 0.010*\"age\"\n",
      "\n",
      "Score: 0.01250387355685234\t \n",
      "Topic: 0.026*\"kill\" + 0.022*\"die\" + 0.018*\"coast\" + 0.018*\"shoot\" + 0.017*\"miss\" + 0.016*\"crash\" + 0.015*\"attack\" + 0.014*\"gold\" + 0.014*\"dead\" + 0.013*\"island\"\n",
      "\n",
      "Score: 0.01250387355685234\t \n",
      "Topic: 0.040*\"police\" + 0.027*\"case\" + 0.026*\"charge\" + 0.026*\"court\" + 0.020*\"death\" + 0.020*\"murder\" + 0.017*\"face\" + 0.013*\"jail\" + 0.013*\"people\" + 0.012*\"arrest\"\n",
      "\n",
      "Score: 0.01250387355685234\t \n",
      "Topic: 0.056*\"trump\" + 0.028*\"test\" + 0.020*\"tasmania\" + 0.015*\"morrison\" + 0.014*\"drum\" + 0.012*\"premier\" + 0.011*\"save\" + 0.010*\"race\" + 0.009*\"show\" + 0.009*\"park\"\n",
      "\n",
      "Score: 0.01250387355685234\t \n",
      "Topic: 0.019*\"bushfire\" + 0.016*\"speak\" + 0.015*\"final\" + 0.013*\"beat\" + 0.013*\"street\" + 0.012*\"season\" + 0.012*\"wall\" + 0.011*\"john\" + 0.010*\"young\" + 0.010*\"continue\"\n",
      "\n",
      "Score: 0.01250387355685234\t \n",
      "Topic: 0.026*\"donald\" + 0.025*\"south\" + 0.024*\"news\" + 0.017*\"north\" + 0.016*\"perth\" + 0.015*\"time\" + 0.013*\"victorian\" + 0.013*\"return\" + 0.012*\"interview\" + 0.012*\"west\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[4310]], key = lambda tup: -1 * tup[1]):\n",
    "    print('\\nScore: {}\\t \\nTopic: {}'.format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.5778061747550964\t \n",
      "Topic: 0.017*\"news\" + 0.010*\"rural\" + 0.010*\"election\" + 0.008*\"government\" + 0.007*\"federal\" + 0.007*\"wednesday\" + 0.007*\"thursday\" + 0.007*\"business\" + 0.007*\"national\" + 0.007*\"john\"\n",
      "\n",
      "Score: 0.177941232919693\t \n",
      "Topic: 0.013*\"hour\" + 0.009*\"david\" + 0.008*\"pandemic\" + 0.008*\"march\" + 0.008*\"plead\" + 0.007*\"june\" + 0.006*\"dollar\" + 0.006*\"reopen\" + 0.005*\"sunday\" + 0.005*\"suicide\"\n",
      "\n",
      "Score: 0.15668842196464539\t \n",
      "Topic: 0.026*\"trump\" + 0.012*\"drum\" + 0.010*\"australia\" + 0.009*\"world\" + 0.008*\"scott\" + 0.008*\"final\" + 0.008*\"tuesday\" + 0.007*\"league\" + 0.006*\"cricket\" + 0.006*\"test\"\n",
      "\n",
      "Score: 0.012510434724390507\t \n",
      "Topic: 0.006*\"kohler\" + 0.006*\"government\" + 0.006*\"jam\" + 0.005*\"people\" + 0.005*\"school\" + 0.005*\"coronavirus\" + 0.005*\"online\" + 0.005*\"territory\" + 0.005*\"sell\" + 0.005*\"smith\"\n",
      "\n",
      "Score: 0.012510214000940323\t \n",
      "Topic: 0.009*\"morrison\" + 0.009*\"health\" + 0.009*\"care\" + 0.007*\"age\" + 0.007*\"coronavirus\" + 0.006*\"mental\" + 0.005*\"government\" + 0.005*\"legal\" + 0.004*\"say\" + 0.004*\"remote\"\n",
      "\n",
      "Score: 0.01250902097672224\t \n",
      "Topic: 0.023*\"coronavirus\" + 0.015*\"covid\" + 0.012*\"donald\" + 0.009*\"queensland\" + 0.009*\"restrictions\" + 0.008*\"victoria\" + 0.006*\"rain\" + 0.006*\"australia\" + 0.006*\"farmers\" + 0.006*\"drought\"\n",
      "\n",
      "Score: 0.012508989311754704\t \n",
      "Topic: 0.015*\"country\" + 0.015*\"coast\" + 0.013*\"live\" + 0.010*\"gold\" + 0.010*\"north\" + 0.008*\"christmas\" + 0.007*\"turnbull\" + 0.007*\"peter\" + 0.007*\"alan\" + 0.007*\"korea\"\n",
      "\n",
      "Score: 0.012508589774370193\t \n",
      "Topic: 0.015*\"interview\" + 0.011*\"friday\" + 0.010*\"finance\" + 0.010*\"update\" + 0.008*\"violence\" + 0.007*\"history\" + 0.007*\"grandstand\" + 0.007*\"domestic\" + 0.007*\"extend\" + 0.006*\"october\"\n",
      "\n",
      "Score: 0.012508504092693329\t \n",
      "Topic: 0.010*\"coronavirus\" + 0.009*\"monday\" + 0.008*\"michael\" + 0.008*\"miss\" + 0.007*\"search\" + 0.007*\"lockdown\" + 0.006*\"covid\" + 0.006*\"vaccine\" + 0.006*\"stories\" + 0.005*\"september\"\n",
      "\n",
      "Score: 0.012508397921919823\t \n",
      "Topic: 0.016*\"police\" + 0.015*\"charge\" + 0.013*\"crash\" + 0.012*\"murder\" + 0.012*\"kill\" + 0.011*\"woman\" + 0.010*\"court\" + 0.009*\"shoot\" + 0.008*\"arrest\" + 0.008*\"die\"\n"
     ]
    }
   ],
   "source": [
    "# Performance evaluation by classifying sample document using the LDA TF-IDF model\n",
    "\n",
    "for index, score in sorted(lda_model_tfidf[bow_corpus[4310]], key = lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3823586702346802\t Topic: 0.043*\"coronavirus\" + 0.030*\"government\" + 0.025*\"covid\" + 0.015*\"restrictions\" + 0.015*\"rise\"\n",
      "Score: 0.2578660845756531\t Topic: 0.022*\"market\" + 0.021*\"record\" + 0.019*\"year\" + 0.013*\"care\" + 0.012*\"years\"\n",
      "Score: 0.21968549489974976\t Topic: 0.026*\"kill\" + 0.022*\"die\" + 0.018*\"coast\" + 0.018*\"shoot\" + 0.017*\"miss\"\n",
      "Score: 0.02001577988266945\t Topic: 0.056*\"trump\" + 0.028*\"test\" + 0.020*\"tasmania\" + 0.015*\"morrison\" + 0.014*\"drum\"\n",
      "Score: 0.02001461572945118\t Topic: 0.037*\"say\" + 0.025*\"victoria\" + 0.022*\"health\" + 0.019*\"change\" + 0.018*\"adelaide\"\n",
      "Score: 0.02001415751874447\t Topic: 0.059*\"australia\" + 0.034*\"coronavirus\" + 0.024*\"australian\" + 0.023*\"china\" + 0.020*\"sydney\"\n",
      "Score: 0.02001303620636463\t Topic: 0.036*\"queensland\" + 0.027*\"election\" + 0.022*\"live\" + 0.013*\"federal\" + 0.012*\"work\"\n",
      "Score: 0.020010707899928093\t Topic: 0.040*\"police\" + 0.027*\"case\" + 0.026*\"charge\" + 0.026*\"court\" + 0.020*\"death\"\n",
      "Score: 0.020010707899928093\t Topic: 0.019*\"bushfire\" + 0.016*\"speak\" + 0.015*\"final\" + 0.013*\"beat\" + 0.013*\"street\"\n",
      "Score: 0.020010707899928093\t Topic: 0.026*\"donald\" + 0.025*\"south\" + 0.024*\"news\" + 0.017*\"north\" + 0.016*\"perth\"\n"
     ]
    }
   ],
   "source": [
    "# Testing model on unseen document\n",
    "\n",
    "unseen_document = 'How a pentagon deal became an indentity crisis for Google'\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key = lambda tup: -1 * tup[1]):\n",
    "    print('Score: {}\\t Topic: {}'.format(score, lda_model.print_topic(index,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
